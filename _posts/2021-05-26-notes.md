---
layout: post
title: "Model Serving Performance Test - A Deep Dive"
categories: tech
author: "Yizheng Huang"
---

### Table of Contents

- [Challenge of Model Deployment](#challenges-of-model-deployment)
- [Improved Performance Testing](#improved-performance-testing)
- [More Comprehensive Analysis Methods](#more-comprehensive-analysis-methods)


In real-world production environments, people care greatly about the online service performance of models (e.g., latency, throughput) and the degree of hardware utilization (e.g., compute utilization rate, memory usage), as well as the friendliness to business logic developers (e.g., system API design).

Therefore, optimizing inference before model deployment has recently become a hot research area. This mainly includes pruning, quantization, and various compilation optimizations, which are optimizations applied to the model itself. However, the performance of the entire serving system is not solely determined by the model itself. The performance of the entire system is affected by the model, the deployment framework, the hardware, and the business logic. Therefore, it is necessary to conduct comprehensive performance testing on the entire system to obtain a more scientific understanding of the system's performance.

### The Typical Model Serving Workflow

Let's take a look at the typical model serving workflow with a client-server architecture:

![](https://s2.loli.net/2024/12/14/CSxYNUAI8eTOiJr.png)

The model checkpoints/weights are usually stored in a model repository, and the model serving framework loads the model from the repository to the target hardware. For example, from the local disk to the CPU memory or from the CPU memory into the GPU memory. After the serving framework is initialized, and the model is loaded into the framework. The serving framework is responsible for managing the model's life cycle, including managing the inference workers/jobs and the hardware resources, and providing an API for the client to send requests.

When the serving servers are ready, we can use the client to send requests for inference. In the client-side, it may include a client-side request processing and batching. Usually, putting data processing in the client-side will require more processing time since the client-side is usually a mobile device or a web browser with limited computing resources. But in term of protecting user privacy, it is a good practice to do so.

When the server receives the request, it puts the request into a queue and waits for the inference worker to process it. The inference worker will load the model from the memory and execute the model inference. After the inference is done, the worker will send the result back to the client. Some serving systems (e.g., Nvidia Triton Inference Server, Tensorflow-serving) support dynamic batching, which introduces the batching process in the server-side.

### Metrics for Model Serving

Now, we understand the typical model serving workflow. Let's talk about the metrics for model serving, the metrics to measure the performance can be categorized into two categories: static metrics and dynamic metrics.

#### Static Metrics

Static metrics are metrics that can be calculated theoretically or from the model and hardware specifications. These metrics include:

- Model Tasks: Classification, Detection, Segmentation, etc.
- Model complexity: FLOPs (time), memory footprint (space)
- Hardware capability: FLOPS (theoretical compute capability), memory, bandwidth

#### Dynamic Metrics

Dynamic metrics are metrics that can only be obtained through real-world performance profiling. These metrics include:

**For model deployment framework (software):**

- Latency (P50, P95, P99) (ms)
- Throughput (varying batch sizes) (req/sec)

**For model acceleration hardware:**

- Average hardware utilization (%)
- Average memory utilization (%)
- Energy consumption per inference (J)
- Carbon emissions per inference (mg)
- Cold start time (sec)
- System startup time (sec)

**For the model service pipeline:**

- Pre-processing latency (ms)
- Post-processing latency (ms)
- Transmission time (ms)

Above metrics are mainly for serving AI models on the cloud. Similarly, for mobile devices (edge devices), inference systems pay more attention to model storage occupancy and battery consumption, so energy consumption and hardware utilization during inference should be the focus. Different systems require different analyses.

There are many excellent works for measuring the Deep learning model serving performance, including [NVIDIA Model Analyzer](https://github.com/triton-inference-server/model_analyzer), [MLPerf](https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/), [AIBench](https://github.com/RedisAI/aibench), etc. 

### Obtaining Dynamic Metrics

To obtain dynamic metrics, we need to design a profiling tool that can simulate real-world scenarios. Based on the serving pipeline we metioned above, we can also draw a simple diagram for it:

![](https://s2.loli.net/2024/12/14/XGpE4BRcmofgwlV.png)

We need to collect the dynamic metrics from both client and server sides. The server-side metrics are more "fine-grained" than the client-side metrics, where for end-to-end metrics, we need to obtain them from the client. Some other metrics need to be aggregated from both sides:

- Latency (P50, P95, P99) with percentile
- Tail latency/Distribution
- Throughput (varying batch sizes)
- Power consumption per inference
- Carbon emissions per inference
- Cloud cost per inference (if deployed on public cloud like AWS, GCP, Azure)

To compute the above metrics, we require the client to store the timestamp when sending the request and receiving the response for each request. The server-side needs to store the timestamp when receiving the request, processing the request, and sending the response. The server-side also needs to store the timestamp when loading the model, initializing the model, and executing the model inference.

Throughput can be simplify calculated by the average latency of all the requests:

$$ Throughput = \frac{1}{\text{Average Latency}} $$

If the requests are batched, the throughput can be calculated by the average latency of all the batches:

$$ Throughput = \frac{Batch Size}{\text{Average Latency of Batches}} $$

For the power consumption and carbon emissions, we need to check the device power meter and compute with the average serving latency. And the carbon emissions can be calculated by the power consumption and the carbon intensity of the electricity, here we recommend using an open-source tool [Carbon Tracker](https://github.com/lfwa/carbontracker). 


### Simulating Real-world Scenarios

A lot of people cannot distinguish between monitoring and performance profiling. The key different between them is that monitoring is a passive process, while performance profiling is an active process with generated workloads. So how to generate the testing workload that can simulate the real-world scenarios, and gather insights for both software and hardware optimizations?

Here are five different ways to generate the inference workload:

- High workload in a short time, testing the system's robustness
- High workload over a continuous period, examining the system's tail latency
- Blocking-style (possibly with multiple concurrencies) fixed-quantity requests, observing hardware utilization and model performance
- Long-term system testing based on workload generated from traces (e.g., Poisson distribution generation)
- Utilization-awared workload generation, examining the hardware capability

![](https://s2.loli.net/2024/12/14/FufY3O6bDxTcCPN.png)

#### Block Requests Sending

The term "block requests sending" means sending thye next request only after the previous request is processed. There is an example Python code for block requests sending with `1000` requests:

```python
import time

for i in range(1000):
    start = time.time()
    # send request
    response = send_request(data)
    end = time.time()
    print(f"Request {i} latency: {end - start}")
```

We can also send requests within a fixed time window, for example, sending `1000` requests within `10` seconds. 

For different serving frameworks and hardwares, the block sending only get some insights when comparing with different settings. For example, using Nvidia V100 GPU and Nvidia T4 GPU, we can compare the latency and throughput with block sending requests. However, it fails to simulate the real-world scenarios, since the real-world scenarios are usually with multiple concurrencies.

#### Multiple Concurrencies Requests Sending

Different from using a single client to send requests, we can use multiple clients to send requests in a blocked way to increase the utilization of the server-side resources. Here is an example Python code for sending requests with multiple concurrencies:

```python
import time
import threading

def send_request():
    for i in range(1000):
        start = time.time()
        # send request
        response = send_request(data)
        end = time.time()
        print(f"Request {i} latency: {end - start}")

threads = []
for i in range(10):
    t = threading.Thread(target=send_request)
    threads.append(t)
    t.start()

for t in threads:
    t.join()
```

The above code will create `10` threads to send requests, and each thread will send `1000` requests. The server-side will receive `10000` requests in total. This can simulate the real-world scenarios with multiple concurrencies. One thing in the above implementation needs to be carefully considered is the time computing in each thread since the "thread time" is not the same as the "real time". And you also need a callback function to trace the latency of each requests with different concurrencies.

The advantage of using multiple concurrencies requests sending is that you can increase the GPU utilization and CPU utilization by setting the number of concurrencies. However, if you want to measure the peak throughtput of a specific hardware with `100%` utilization, it is hard to set the number of concurrencies.

And to test the robustness of the system, you can send all the requests in a short time by calling asynchronous requests sending. Here is an example Python code for sending requests with asynchronous requests sending:

```python
import time

def send_request():
    for i in range(1000):
        # send request
        send_time = time.time()
        response = sync_send_request(data)
```

You need to collect the receive time of the reponse in the asynchronous callback function. By using this "burst" sending, you can test the system's robustness and the system's tail latency, since the request queue will be full in a short time.

#### Poisson Distribution Generation

The Poisson distribution is a probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space. The Poisson distribution can be used to simulate the real-world scenarios, since the requests are not sent in a fixed time interval. Here is an example Python code for generating Poisson distribution requests:

```python
import random

def gen_arrival_time(duration=60 * 1, arrival_rate=5, seed=None):
    start_time = 0
    arrive_time = []

    if seed is not None:
        random.seed(seed)

    while start_time < duration:
        start_time = start_time + random.expovariate(arrival_rate)
        arrive_time.append(start_time)

    return arrive_time
```

You can set the `arrival_rate` to control the requests' arrival rate. And you can set the `duration` to control the total time of the requests. Some company have their server workload traces, and you can use the traces to generate the Poisson distribution requests to simulate the real-world scenarios.

#### Utilization-awared Workload Generation

Tail latency is an important metric for the system's performance. Let's say you sent a number of requests that are over the server's capability, many requests will be stored in the request queue, results in a long waiting time and the decrease of the average throughput. To get the accurate maximum throughput, you need to reduce the queuing time but utilize the resources to the maximum by setting the number of concurrencies based on the resource utilization.

For example, if you want to test the system's performance when the GPU utilization is `80%`, you can first set the concurrency number to 1 and increase the concurrency number until gradually when the GPU utilization stable at `80%`. Although the GPU utilization will be delayed until the client receives, results in an scillation of the GPU utilization, you can get an accurated average value by testing the performance with a longer time.

### Performance Analysis

After gathering the dynamic metrics, we need to analyze the performance data to get insights for both software and hardware optimizations. Here are some common performance analysis methods:

- KDE (Kernel Density Estimation) and CDF (Cumulative Distribution Function) for latency distribution
- Heatmap for Different Machine Learning Models
- Roofline Model Analysis
- Time Series Analysis
- Pipeline Analysis



<!-- ![The system's inference latency tailing under different batch sizes and workloads](https://pic2.zhimg.com/v2-6d1c40d781c3b7c263c3f302b33a5a69_r.jpg)

Since inference often occurs online, in many scenarios, batch prediction is not feasible. No matter how many concurrent requests there are, hardware resources may not be fully utilized. Currently, TensorFlow Serving and Nvidia Triton Inference Server support dynamic batching, and it is believed that more inference frameworks will support it in the future. Benchmark tools should cover testing methods for requests with varying batch sizes (based on personal experience, tuning dynamic batching parameters incorrectly can decrease system throughput. As shown below, the left diagram is Nvidia Triton Inference Server, and the right is a PyTorch Dynamic Batching implementation on Flask. If the total number of requests received within the maximum waiting time for the request queue does not meet the target batch, the benefits of batch predict may not offset the waiting time's overhead, resulting in an actual performance drop).

![Dynamic Batching](https://pic2.zhimg.com/v2-79b266dd59e7ad28a6b8d68b451449ad_r.jpg)

(the left-side is TensorFlow Serving, the right-side is Dynamic Batching on Nvidia Triton Inference Server)

### More Comprehensive Analysis Methods

After scientifically and uniformly testing performance, how to analyze it is also an important step in improving existing inference systems. Common analysis methods include data visualization and horizontal and vertical comparisons of data across different dimensions. This section won’t go into details, as the primary purpose of benchmarking tools is to obtain more scientific performance data.

Here's an example of an analysis: performing a Roofline Model analysis by combining dynamic data obtained from benchmarking with calculated theoretical values. Below is a Roofline Model of some common models:

![Roofline Models](https://pic4.zhimg.com/v2-16115c0837b38b2fcc4f0f3a16296d11_r.jpg)

The red ceiling line represents the theoretical bandwidth and compute capability of Tesla V100. The model's compute intensity (horizontal axis) is calculated from the model's theoretical FLOPs and memory footprint. The vertical axis of the model is obtained from the benchmark system by calculating the model throughput (peak QPS at batch size 1). All models (TF-SavedModel) are deployed through TensorFlow Serving.

The left area of the graph indicates models limited by hardware bandwidth, while the red area on the right indicates models limited by hardware computing capability. It’s easy to see that MobileNet, which has low compute intensity, is primarily affected by hardware bandwidth, while VGG, with high compute intensity, is mainly constrained by hardware compute capability.

The fact that none of the points reach the theoretical ceiling is because the throughput measured here includes simple data processing, I/O, and transmission time. These are often overlooked but are crucial parts of the serving pipeline improvement. In actual inference processes, various other factors may affect the system's ability to reach the theoretical optimum.

A comprehensive serving system performance analysis should include the entire pipeline and analyze both theoretical data and dynamic results. For specific scenarios, it may be appropriate to abandon certain seemingly important metrics and choose the optimal online deployment strategy for different configurations. For example, for applications where the entire model inference is executed in the backend, pre-processing and post-processing operations may affect the system's performance in concurrent inference scenarios. As shown below, for instance, inference using the same number of RTX 2080Ti GPUs shows significantly lower performance on a single machine than on different machines. Observing system resource usage reveals that the GPU is not fully utilized; instead, CPU usage is maxed out due to data pre-processing, causing a performance bottleneck. If the system is deployed on cloud services, increasing CPU resources appropriately can break this performance bottleneck.

![CPU-caused performance bottleneck](https://picx.zhimg.com/v2-c62615ef265ceb041ca9ecb899337f1f_r.jpg)

In summary, conducting performance testing on an ML serving service requires considering various aspects and using scientific methods to examine and analyze data. In actual production environments, many software and hardware resources may not work as officially described. At such times, comprehensive system analysis and inspection can both reveal shortcuts for improving performance bottlenecks and provide insights for future research. -->
