---
layout: post
title: "Reflections on LLM Agent Development"
categories: tech
author: "Yizheng"
---


#### LLM Application Development: Still Software Engineering at Its Core

I recall 2018, during the boom of deep learning and computer vision, when university dorms were filled with copies of Li Hang's Statistical Learning Methods and Zhou Zhihua's watermelon book. Many students were working hard to learn AI, with the goal of becoming machine learning engineers (MLEs). At that time, traditional software development skills didn't seem to overlap much with MLE skills, creating two distinct groups: those studying math and Python, and those focusing on frontend and backend development for employment. In universities, these groups often kept to themselves, with mutual disdain—software engineering students thought AI students lacked engineering skills, while AI students dismissed software engineers as mere coders with no real understanding of AI research.

Interestingly, this gap started to close significantly in the industry, where companies wanted MLEs with strong engineering skills, since most AI efforts were aimed at serving products and services. With the arrival of LLMs, I've noticed this gap shrinking even faster—as OpenAI's careers page states, "Our engineers do a lot of research, and our researchers do a lot of engineering." To build a good LLM application, developers don't need to focus on LLMs themselves; often, they don't even need to worry about model deployment or resource allocation. What's more important is building a stable and intelligent software system around the LLM—these systems are referred to as [Compound AI Systems](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/). Beyond the traditional frontend and backend, a complete LLM application might integrate a vector database, search engine, and business toolchain. Strong backend development and architecture skills are the touchstone for leveraging LLM capabilities, while needs like model structure modification, fine-tuning, and deployment optimization will gradually diminish within companies.

![Compound AI System (from UC Berkeley BAIR)](https://s2.loli.net/2024/08/14/Bj4sMpvt5uGbImR.png)

#### Small Models Won't Disappear; They'll Be Part of the System

LLMs are popular, but that doesn't mean traditional small models and deep learning methods will vanish overnight. For many specific tasks, well-tuned small models can outperform large models, particularly due to their ease of deployment, stronger data privacy, and faster inference speeds. Many companies will still opt for small models for such tasks. On the other hand, for more complex and intelligence-demanding tasks, LLMs are clearly a better choice. Therefore, mature business systems won't use only small models or only large models; instead, they'll employ a sophisticated combination of both.

For example, if I want to build an application to accurately summarize web pages, directly using an LLM might not yield good results, while small models might lack the summarization power needed. A better approach might be to first have a small model extract keywords from a messy webpage, and then input those keywords into an LLM for summarization. This combined approach yields better results, while balancing cost and speed considerations. Thus, in the foreseeable future, large models won't dominate the entire AI landscape; instead, infrastructure and application architectures involving a combination of large and small models will mature over time. For students, it is wise to not merely chase trends but start with small models and gradually build a comprehensive AI knowledge base.

#### Function Calling in LLMs

Another key insight about LLMs: they aren't as powerful as you might think. Many people, upon witnessing the rise of ChatGPT, felt the world was on the brink of transformation. However, once you begin developing an AI agent, you constantly oscillate between thinking, "It's really dumb" and "It's incredibly smart." The problem is, no real business can afford to deal with a model that is sometimes smart and sometimes not—a single failure might cost you user trust.

A practical approach to LLM agent development is to avoid asking the LLM to do too much. This seems counterintuitive, since as the smartest part of a composite system, one might naturally assign as many tasks as possible to the LLM. However, in reality, current LLM capabilities aren't reliable enough to handle different types of tasks with high accuracy every single time. Thus, we employ fine-tuned language models that support function calling (e.g., GPT-4-o, Llama-3-Instruct-function-calling), paired with pre-implemented functions to realize business logic—turning LLMs into "tool users" rather than tools themselves.

Consider the example of asking ChatGPT about the weather in Los Angeles today. It may generate inaccurate information, but if a function to query the weather is pre-written and the LLM is made to call it, the returned result will always be accurate. Function calling is crucial in LLM agent development—a good LLM application developer needs not only to converse effectively with LLMs but also to decide what tasks require function completion and what tasks can be handled by the LLM itself, integrating them into business logic accordingly.

#### RAG and Search: Handling Hallucinations

RAG (Retrieval-Augmented Generation) is a popular concept in LLM agent development, but its principle is quite simple. For example, if you ask ChatGPT:

```
Can you solve question five from UCLA CS111 homework? The question is as follows:

....

```

ChatGPT may not give the correct answer, but if you provide relevant course references as context:

```
Can you solve question five from UCLA CS111 homework? The question is as follows:

...

References:
...

```

The response is usually better—this is a basic example of RAG: introducing external knowledge into the LLM interaction. However, two main issues arise:

- The LLM's context window is too small to fit enough external data. UCLA CS111's reading materials are extensive, and LLMs don't support putting entire books into a conversation.
- Providing too much data causes the LLM to become unstable and hallucinate. It can lose focus, forget your original question, and generate unrelated or incorrect responses.

Many vector databases provide effective tools for LLM RAG, allowing user and personal data to be encoded, stored, and efficiently retrieved to augment LLMs. Additionally, LLMs often integrate search functions to incorporate external knowledge—these are fundamentally similar, and hallucinations are a common problem with both methods. This difficulty is a key reason why LLMs face challenges in achieving widespread industrial adoption—businesses cannot afford hallucinations, and many lack the data and resources to fine-tune large models, while being unable to ignore the improvements from RAG/search.

Hence, LLM developers face a new challenge: effectively handling the external data fed into LLMs. It requires a careful balance—building a performant LLM agent isn't like training a massive transformer model where more data is better; rather, it's about carefully evaluating the positive and negative impacts of the data introduced and creating a good data processing pipeline in Compound AI Systems. Returning to the earlier example, instead of dumping the entire CS111 textbook into an LLM conversation, searching for the most relevant content with a vector database and providing that as a reference can help reduce context window usage and improve response quality. Moreover, for some conceptual common knowledge, LLMs likely don't need explicit reference—instead, using wrong answers and specific course examples as references might yield better results.

For a developer familiar with LLM agent development, efficiently processing and filtering data based on specific use cases will be a crucial skill.

#### Multiple-Agent and Human Interaction Systems

In addition to interactions between small and large models, handling complex business logic often requires multiple agents to work together. In the [gpt-researcher](https://github.com/assafelovic/gpt-researcher) project, for example, LLMs need to play roles like researcher, reviewer, editor, and publisher to complete an end-to-end research task. Each role has a different system prompt and uses different functions/tools, interacting continuously until the user is satisfied. We've recently built a similar end-to-end system, [MLE-Agent](https://github.com/MLSysOps/MLE-agent), which performs tasks like paper retrieval, coding, testing, and report generation based on the needs of data scientists and researchers. Behind the scenes, it involves advisors, planners, developers, and debuggers working in tandem.

These systems can be divided into two types:

- Multi-agent systems with fixed processes
- Multi-agent systems with dynamic processes

Writing a research paper is a relatively fixed process, from conducting a literature review to verifying an idea to writing the paper—each phase is handled by an independent agent in a set sequence. On the other hand, in LLM-based coding applications like [gpt-pilot](https://github.com/Pythagora-io/gpt-pilot) and [OpenDevin](https://github.com/OpenDevin/OpenDevin), the system dynamically calls different agents based on user requirements and real-time feedback to complete the final coding task. Such applications often require a graph to manage and call different agents, which is one reason the LangChain community recently shifted its focus from an LLM development toolkit to projects like [LangGraph](https://github.com/langchain-ai/langgraph).


#### Testing Processes and Safety Regulations

Finally, it's impossible to discuss LLM development without mentioning testing processes and generative AI safety—issues that have grown in significance as LLM applications have become more widespread. For developers, the key to LLM applications is how to detect and track LLM inputs and outputs. This helps in adjusting and testing different prompt versions to achieve better intelligence, as well as performing effective version rollbacks when problems occur in production. Many startups are exploring these applications—for example, [LangFuse](https://langfuse.com/) and [BreezeML](https://breezeml.ai/). Such testing and tracking platforms are expected to proliferate in the coming years. For enterprises, there may also be privacy concerns when adopting third-party LLM services (e.g., OpenAI). How to detect personally identifiable information (PII) in LLM input, and how to efficiently mask such data without affecting model performance, are critical considerations.

From the LLM perspective, how to evaluate outputs for biases or harmful content is also a major factor impacting adoption. A common approach is to use human-annotated datasets to test LLM safety and compute predefined metrics like hallucination, correctness, conciseness, and toxicity. Internally, companies often need to build LLM quality evaluation datasets tailored to their own use cases. These datasets can be simple, containing model inputs and outputs along with scores for various metrics.