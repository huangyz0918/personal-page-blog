---
layout: post
title: "Toward Parameter-efficient LLM Fine-tuning"
categories: tech
author: "Yizheng"
---


### Why we need parameter-efficiency?

随着现在大语言模型 (LLM) 的快速发展，深度学习模型的参数量在极速增长，这种增长从系统角度带来了巨大的挑战：

- Full fine-tuning 在消费级硬件上逐渐变得不可能。 由于模型参数量的增长，普通消费硬件的内存以及算力都不支持全模型微调。
- 下游任务变多导致存储浪费。随着大语言模型的泛化能力的提升，一个 pre-train 模型往往能够适应多个下游任务，但针对不同下游任务 fullly fine-tuned 的模型参数是和 pre-trained 模型一样的，这就导致了某个下游任务的模型参数是冗余的，占用巨量的存储和部署资源。



